<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="google-site-verification" content="CUG7VaEEdOQmXyIwy21RkN7y0H5HmjjmV1viLfi_HLw" />

  <title>Cheul Young Park</title>
  <meta name="description" content="">

  

  <!-- <link rel="shortcut icon" href="/assets/img/favicon.ico"> -->

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/">
  <!-- <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;1,400;1,700&family=Source+Sans+Pro:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet"> -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0HBLGM34YQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0HBLGM34YQ');
  </script>
  
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <!-- <span class="site-title">
        
        <a class="page-link" href="/" style="font-size:1.25rem">
          <strong>Cheul</strong> Park
        </a>
    </span> -->

    <!-- <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label> -->

      <div class="trigger">
        <!-- About -->
        <!-- <a class="page-link" href="/">About</a> -->

        <!-- Blog -->
        <!-- <a class="page-link" href="/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/cheulyoung_park-short_cv.pdf">CV</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h2 class="post-title">Cheul Young Park</h2>
    <h5 class="post-description">Last updated: 23.05.06</h5>
  </header>

  <article class="post-content Cheul Young Park clearfix">
    

  <div class="profile col one left">
    
      <img class="one" src="/assets/img/apkc.png">
    
    
  </div>
  <div class="profile col two right">
    <p>I am a Data Manager at <strong><a href="https://www.upstage.ai/">Upstage</a></strong>.
I work on:</p>
<ul>
  <li>automating Data Management Operations (DMOps),</li>
  <li>and developing LLM-based applications.</li>
</ul>

<p>Previously, I was:</p>
<ul>
  <li>an AI Researcher at <a href="https://silvia.io/">Silvia Health</a>. I trained models to detect cognitive impairments from speech.</li>
  <li>a Master’s student in <a href="https://ic.kaist.ac.kr/">Interactive Computing Lab</a> at KAIST. I worked on:
    <ul>
      <li><a href="https://github.com/cheulyop/AdaptiveESM">AdaptiveESM</a> – an active learning approach towards a balanced and efficient collection of emotions in the wild.</li>
      <li><a href="https://doi.org/10.5281/zenodo.3762961">K-EmoCon Dataset</a> – a multimodal, biometric sensor dataset for recognizing emotions in the wild.</li>
    </ul>
  </li>
</ul>

<p><strong>CV:</strong>
[<a class="page-link" href="/assets/pdf/cv-jan22.pdf">pdf</a>] (last updated: <em>Jan. ‘22</em>)</p>

  </div>



<!-- about page without profile pic -->
<!-- ---
layout: page
---


  <div class="profile col one left">
    
      <img class="one" src="/assets/img/apkc.png">
    
    
  </div>


<p>I am a Data Manager at <strong><a href="https://www.upstage.ai/">Upstage</a></strong>.
I work on:</p>
<ul>
  <li>automating Data Management Operations (DMOps),</li>
  <li>and developing LLM-based applications.</li>
</ul>

<p>Previously, I was:</p>
<ul>
  <li>an AI Researcher at <a href="https://silvia.io/">Silvia Health</a>. I trained models to detect cognitive impairments from speech.</li>
  <li>a Master’s student in <a href="https://ic.kaist.ac.kr/">Interactive Computing Lab</a> at KAIST. I worked on:
    <ul>
      <li><a href="https://github.com/cheulyop/AdaptiveESM">AdaptiveESM</a> – an active learning approach towards a balanced and efficient collection of emotions in the wild.</li>
      <li><a href="https://doi.org/10.5281/zenodo.3762961">K-EmoCon Dataset</a> – a multimodal, biometric sensor dataset for recognizing emotions in the wild.</li>
    </ul>
  </li>
</ul>

<p><strong>CV:</strong>
[<a class="page-link" href="/assets/pdf/cv-jan22.pdf">pdf</a>] (last updated: <em>Jan. ‘22</em>)</p>
 -->
  </article>

  
    <div class="news">
  <h1 class="post-title">Updates</h1>
  
    <table>
    
    
      <tr>
        <td class="date">📌 Dec 9, 2021</td>
        <td class="announcement">
          
            A new paper <a href="https://dl.acm.org/doi/10.1109/EMBC46164.2021.9630252">“<em>Arousal-Valence Classification from Peripheral Physiological Signals Using Long Short-Term Memory Networks</em>”</a> has been published on IEEE EMBC.

          
        </td>
      </tr>
    
      <tr>
        <td class="date">📌 Nov 16, 2021</td>
        <td class="announcement">
          
            A new paper has been conditionally accepted with minor revisions to <a href="https://chi2022.acm.org/">CHI’22</a>!

          
        </td>
      </tr>
    
      <tr>
        <td class="date">📌 Feb 1, 2021</td>
        <td class="announcement">
          
            Joined the amazing team at <a href="https://silvia.io">Silvia Health</a> as an AI Researcher 🎉

          
        </td>
      </tr>
    
      <tr>
        <td class="date">📌 Dec 23, 2020</td>
        <td class="announcement">
          
            Successfully defended my master’s thesis <a href="/assets/pdf/thesis2021towards.pdf">“<em>Towards Adaptive Sampling of Emotions in the Wild with Active Learning</em>”</a>

          
        </td>
      </tr>
    
      <tr>
        <td class="date">📌 Nov 22, 2020</td>
        <td class="announcement">
          
            My past work on password sharing was featured in the <a href="https://www.wired.co.uk/">WIRED UK</a> article <a href="https://www.wired.co.uk/article/password-security-sharing">“<em>You need to stop sharing your passwords with your partner</em>”</a> by <a href="https://www.wired.co.uk/profile/victoria-turk">Victoria Turk</a>.

          
        </td>
      </tr>
    
      <tr>
        <td class="date">📌 Sep 8, 2020</td>
        <td class="announcement">
          
            A new paper <a href="https://www.nature.com/articles/s41597-020-00630-y">“<em>K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations</em>”</a> has been published on Nature Scientific Data.

          
        </td>
      </tr>
    
      <tr>
        <td class="date">📌 Sep 7, 2020</td>
        <td class="announcement">
          
            A new paper <a href="https://dl.acm.org/doi/10.1145/3411810">“<em>Hello There! Is Now a Good Time to Talk?: Opportune Moments for Proactive Interactions with Smart Speakers</em>”</a> has been published on IMWUT.

          
        </td>
      </tr>
    
    </table>
  
</div>

  

  
    <div class="publications">
  <h1 class="post-title">Publications</h1>
  
    <header class="pub-heading">
	<h4>Manuscripts &amp; Preprints</h4>
</header>
<h5 class="pub-caption">* Works in this section are subject to changes until their official publication</h5>

<ol class="bibliography"><li>
  
    <abbr>[<a href="https://dl.acm.org/conference/chi" target="_blank">CHI</a>]</abbr>
  


<div id="1">
  
    <span class="title">Understanding Emotion Changes in Mobile Experience Sampling</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.co.kr/citations?hl=en&amp;user=B9HMz0EAAAAJ" target="_blank">Soowon Kang</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                Narae Cha,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://kimauk.github.io/" target="_blank">Auk Kim</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="https://cs.kaist.ac.kr/people/view?idx=617&amp;kind=faculty&amp;menu=172" target="_blank">Uichin Lee</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In CHI 2022, to appear</em>
    
    
    </span>
  

  <span class="links">
  
  
  
  
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  

</div>
</li></ol>

<header class="pub-heading">
	<h4>Refereed Conference &amp; Journal Papers</h4>
</header>

<h3 class="year">2021</h3>
<ol class="bibliography"><li>
  
    <abbr>[<a href="https://ieeexplore.ieee.org/xpl/conhome/1000269/all-proceedings" target="_blank">EMBC</a>]</abbr>
  


<div id="4">
  
    <span class="title">Arousal-Valence Classification from Peripheral Physiological Signals Using Long Short-Term Memory Networks</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                M Sami Zitouni,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://cs.kaist.ac.kr/people/view?idx=617&amp;kind=faculty&amp;menu=172" target="_blank">Uichin Lee</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.com/citations?user=OfAkcXkAAAAJ&amp;hl=en" target="_blank">Leontios Hadjileontiadis</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="https://scholar.google.com/citations?hl=en&amp;user=MpOViugAAAAJ" target="_blank">Ahsan Khandoker</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In 2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</em>
    
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
  
    [<a href="/assets/pdf/zitouni2021arousal.pdf" target="_blank">pdf</a>]
  
  
    [<a href="https://doi.org/10.1109/EMBC46164.2021.9630252" target="_blank">web</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>The automated recognition of human emotions plays an important role in developing machines with emotional intelligence. However, most of the affective computing models are based on images, audio, videos and brain signals. There is a lack of prior studies that focus on utilizing only peripheral physiological signals for emotion recognition, which can ideally be implemented in daily life settings using wearables, e.g., smartwatches. Here, an emotion classification method using peripheral physiological signals, obtained by wearable devices that enable continuous monitoring of emotional states, is presented. A Long Short-Term Memory neural network-based classification model is proposed to accurately predict emotions in real-time into binary levels and quadrants of the arousal-valence space. The peripheral sensored data used here were collected from 20 participants, who engaged in a naturalistic debate. Different annotation schemes were adopted and their impact on the classification performance was explored. Evaluation results demonstrate the capability of our method with a measured accuracy of &gt;93% and &gt;89% for binary levels and quad classes, respectively. This paves the way for enhancing the role of wearable devices in emotional state recognition in everyday life.</p>
  </span>
  

</div>
</li></ol>

<h3 class="year">2020</h3>
<ol class="bibliography"><li>
  
    <abbr>[<a href="https://www.nature.com/sdata/" target="_blank">Sci. Data</a>]</abbr>
  


<div id="3">
  
    <span class="title">K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                Narae Cha,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.co.kr/citations?hl=en&amp;user=B9HMz0EAAAAJ" target="_blank">Soowon Kang</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://kimauk.github.io/" target="_blank">Auk Kim</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.com/citations?hl=en&amp;user=MpOViugAAAAJ" target="_blank">Ahsan Khandoker</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.com/citations?user=OfAkcXkAAAAJ&amp;hl=en" target="_blank">Leontios Hadjileontiadis</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://aliceoh9.github.io/" target="_blank">Alice Oh</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="http://ibrain.kaist.ac.kr/professor/" target="_blank">Yong Jeong</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="https://cs.kaist.ac.kr/people/view?idx=617&amp;kind=faculty&amp;menu=172" target="_blank">Uichin Lee</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Nature Scientific Data</em>
    
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
  
    [<a href="/assets/pdf/k-emocon.pdf" target="_blank">pdf</a>]
  
  
    [<a href="https://doi.org/10.1038/s41597-020-00630-y" target="_blank">web</a>]
  
  
  
  
  
    [<a href="https://doi.org/10.5281/zenodo.3931963" target="_blank">data</a>]
  
  
    [<a href="https://github.com/Kaist-ICLab/K-EmoCon_SupplementaryCodes" target="_blank">code</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recognizing emotions during social interactions has many potential applications with the popularization of low-cost mobile sensors, but a challenge remains with the lack of naturalistic affective interaction data. Most existing emotion datasets do not support studying idiosyncratic emotions arising in the wild as they were collected in constrained environments. Therefore, studying emotions in the context of social interactions requires a novel dataset, and K-EmoCon is such a multimodal dataset with comprehensive annotations of continuous emotions during naturalistic conversations. The dataset contains multimodal measurements, including audiovisual recordings, EEG, and peripheral physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately 10-minute long paired debates on a social issue. Distinct from previous datasets, it includes emotion annotations from all three available perspectives: self, debate partner, and external observers. Raters annotated emotional displays at intervals of every 5 seconds while viewing the debate footage, in terms of arousal-valence and 18 additional categorical emotions. The resulting K-EmoCon is the first publicly available emotion dataset accommodating the multiperspective assessment of emotions during social interactions.</p>
  </span>
  

</div>
</li>
<li>
  
    <abbr>[<a href="https://dl.acm.org/journal/imwut" target="_blank">IMWUT</a>]</abbr>
  


<div id="2">
  
    <span class="title">Hello There! Is Now a Good Time to Talk? Opportune Moments for Proactive Interactions with Smart Speakers</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                Narae Cha,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://kimauk.github.io/" target="_blank">Auk Kim</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.co.kr/citations?hl=en&amp;user=B9HMz0EAAAAJ" target="_blank">Soowon Kang</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                Mingyu Park,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://cs.kaist.ac.kr/people/view?idx=616&amp;kind=faculty&amp;menu=160" target="_blank">Jae-Gil Lee</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.com/citations?user=1cgtVW8AAAAJ&amp;hl=en" target="_blank">Sangsu Lee</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="https://cs.kaist.ac.kr/people/view?idx=617&amp;kind=faculty&amp;menu=172" target="_blank">Uichin Lee</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>
    
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
  
    [<a href="/assets/pdf/hellothere.pdf" target="_blank">pdf</a>]
  
  
    [<a href="https://dl.acm.org/doi/10.1145/3411810" target="_blank">web</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Increasing number of researchers and designers are envisioning a wide range of novel proactive conversational services for smart speakers such as context-aware reminders and restocking household items. When initiating conversational interactions proactively, smart speakers need to consider users’ contexts to minimize disruption. In this work, we aim to broaden our understanding of opportune moments for proactive conversational interactions in domestic contexts. Toward this goal, we built a voice-based experience sampling device and conducted a one-week field study with 40 participants living in university dormitories. From 3,572 in-situ user experience reports, we proposed 19 activity categories to investigate contextual factors related to interruptibility. Our data analysis results show that the key determinants for opportune moments are closely related to both personal contextual factors such as busyness, mood, and resource conflicts for dual-tasking, and the other contextual factors associated with the everyday routines at home, including user mobility and social presence. Based on these findings, we discuss the need for designing context-aware proactive conversation management features that dynamically control conversational interactions based on users’ contexts and routines.</p>
  </span>
  

</div>
</li></ol>

<h3 class="year">2018</h3>
<ol class="bibliography"><li>
  
    <abbr>[<a href="https://dl.acm.org/conference/soups" target="_blank">SOUPS</a>]</abbr>
  


<div id="1">
  
    <span class="title">Share and Share Alike? An Exploration of Secure Behaviors in Romantic Relationships</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://corifaklaris.com/" target="_blank">Cori Faklaris</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://www.siyanz.com/" target="_blank">Siyan Zhao</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="http://www.alexsciuto.com/" target="_blank">Alex Sciuto</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="http://www.lauradabbish.com/" target="_blank">Laura Dabbish</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="http://www.cs.cmu.edu/~jasonh/" target="_blank">Jason Hong</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Fourteenth Symposium on Usable Privacy and Security (SOUPS)</em>
    
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
  
    [<a href="/assets/pdf/soups2018-park.pdf" target="_blank">pdf</a>]
  
  
    [<a href="https://www.usenix.org/conference/soups2018/presentation/park" target="_blank">web</a>]
  
  
  
  
  
  
  
    [<a href="https://www.cylab.cmu.edu/news/2018/09/07-romantic-couples-security.html" target="_blank">press</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Security design choices often fail to take into account users’ social context. Our work is among the first to examine security behavior in romantic relationships. We surveyed 195 people on Amazon Mechanical Turk about their relationship status and account sharing behavior for a cross-section of popular websites and apps (e.g., Netflix, Amazon Prime). We examine differences in account sharing behavior at different stages in a relationship and for people in different age groups and income levels. We also present a taxonomy of sharing motivations and behaviors based on the iterative coding of open-ended responses. Based on this taxonomy, we present design recommendations to support end users in three relationship stages: when they start sharing access with romantic partners; when they are maintaining that sharing; and when they decide to stop. Our findings contribute to the field of usable privacy and security by enhancing our understanding of security and privacy behaviors and needs in intimate social relationships.</p>
  </span>
  

</div>
</li></ol>


  
</div>
  

  
    <div class="research">
  <h1 class="post-title">Research</h1>
  
    <ul>
  <li><strong><a href="http://ic.kaist.ac.kr/wiki/wiki.cgi?Main">Interactive Computing Lab</a></strong>, KAIST KSE (Aug ‘18 ~ Feb ‘21)
    <ul>
      <li>Mobile Computing, Affective Computing (Advised by <a href="http://ic.kaist.ac.kr/wiki/wiki.cgi?UichinLee">Prof. Uichin Lee</a>)</li>
    </ul>
  </li>
  <li><strong><a href="http://cmuchimps.org/">CHIMPS Lab</a></strong>, CMU HCII (Jul ‘17 ~ Jul ‘18)
    <ul>
      <li>Usable Privacy and Security (Advised by <a href="http://www.cs.cmu.edu/~jasonh/">Prof. Jason Hong</a>)</li>
    </ul>
  </li>
  <li><strong><a href="https://www.cmu.edu/dietrich/psychology/infant-language-learning-lab/">Infant Language and Learning Lab</a></strong>, CMU Psychology (May ‘16 ~ Dec ‘16)
    <ul>
      <li>Developmental Psychology, Cognitive Psychology (Advised by <a href="https://www.cmu.edu/dietrich/psychology/people/core-training-faculty/thiessen-erik.html">Prof. Erik Thiessen</a>)</li>
    </ul>
  </li>
</ul>

  
</div>
  

  
    <div class="courses">
  <h1 class="post-title">Teaching</h1>
  
    <!-- <header class="pub-heading">
	<h4>Machine Learning</h4>
</header> -->

<ul>
  <li><a href="https://ita.kaist.ac.kr/"><strong>KAIST IT Academy</strong></a> - <strong>Image Deep Learning with PyTorch</strong> (<ins>Feb ‘20</ins>, co-instructed with <a href="https://linkedin.com/in/jisuhan">Jisu Han</a>)
    <ul>
      <li>A 30-hour course on the basics of deep learning using PyTorch with a focus on image processing, taught in Korean &amp; English.</li>
    </ul>
  </li>
</ul>

<!-- <header class="pub-heading">
	<h4>Programming</h4>
</header> -->

<ul>
  <li><a href="https://ita.kaist.ac.kr/"><strong>KAIST IT Academy</strong></a> - <strong>Python Programming Basics</strong> (<ins>Oct ‘19</ins> &amp; <ins>Jan ‘20</ins>)
    <ul>
      <li>A 30-hour course on the fundamentals of programming with Python, taught in Korean &amp; English.</li>
    </ul>
  </li>
</ul>

  
</div>
  

  
    <div class="social">
  <span class="contacticon center" style="font-size: 16px">
    <a href="mailto:%63%68%65%75%6C%79%6F%70@%67%6D%61%69%6C.%63%6F%6D">email</a>
    <!-- <a href="https://orcid.org/0000-0003-0414-272X" target="_blank" title="ORCID">ORCID</a> -->
    <a href="https://github.com/cheulyop" target="_blank" title="GitHub">github</a>
  </span>

  <!-- <span class="contacticon center" style="font-size: 24px">
    <a href="mailto:%63%68%65%75%6C%79%6F%70@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope fa-sm"></i></a>
    <a href="https://orcid.org/0000-0003-0414-272X" target="_blank" title="ORCID"><i class="ai ai-orcid ai-xs"></i></a>
    
    
    
    <a href="https://github.com/cheulyop" target="_blank" title="GitHub"><i class="fab fa-github fa-sm"></i></a>
    
    
    
    
    
    
  </span> -->

  <div class="col three caption">
    
  </div>
</div>

  

</div>

      </div>
    </div>

    <!-- <footer>

  <div class="wrapper">
    &copy; Copyright 2023 Cheul Young Park.
    
    
        Last updated: 2021-01-11.
    
  </div>

</footer>
 -->

    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>





<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">


<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-163428563-1', 'auto');
ga('send', 'pageview');
</script>



  </body>

</html>
