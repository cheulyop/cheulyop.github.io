<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="google-site-verification" content="CUG7VaEEdOQmXyIwy21RkN7y0H5HmjjmV1viLfi_HLw" />

  <title>Cheul Young Park</title>
  <meta name="description" content="">

  

  <link rel="shortcut icon" href="/assets/img/favicon.ico">

  <link rel="stylesheet" href="/assets/css/main.css">
  <link rel="canonical" href="/">
  <!-- <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;1,400;1,700&family=Source+Sans+Pro:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet"> -->

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-0HBLGM34YQ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-0HBLGM34YQ');
  </script>
  
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <!-- <span class="site-title">
        
        <a class="page-link" href="/" style="font-size:1.25rem">
          <strong>Cheul</strong> Park
        </a>
    </span> -->

    <!-- <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label> -->

      <div class="trigger">
        <!-- About -->
        <!-- <a class="page-link" href="/">About</a> -->

        <!-- Blog -->
        <!-- <a class="page-link" href="/blog/">blog</a> -->

        <!-- Pages -->
        
          
        
          
        
          
        
          
        
          
        

        <!-- CV link -->
        <!-- <a class="page-link" href="/assets/pdf/cheulyoung_park-short_cv.pdf">CV</a> -->

      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h2 class="post-title">Cheul Young Park</h2>
    <h5 class="post-description">cheulyop [at] gmail [dot] com</h5>
  </header>

  <article class="post-content Cheul Young Park clearfix">
    

  <div class="profile col one left">
    
      <img class="one" src="/assets/img/profile.png">
    
    
  </div>
  <div class="profile col two right">
    <p>My research interests include Affective Computing &amp; Human-Computer Interaction. I am also interested in applying Machine Learning to understand the relationship between our affective states and physiological signals and exploring how we can use that knowledge for real-world applications such as digital healthcare.</p>

<p>I am currently a Master’s student in the <a href="https://kse.kaist.ac.kr/">Graduate School of Knowledge Service Engineering</a> at <a href="https://www.kaist.ac.kr/en/">KAIST</a>. I recently defended my thesis and will be graduating this February.</p>

<p>I am a member of the <a href="http://ic.kaist.ac.kr/wiki/wiki.cgi?Main">Interactive Computing Lab</a> at KAIST. During my time here, I worked on the following projects:</p>
<ul>
  <li><a href="https://github.com/cheulyop/AdaptiveESM">AdaptiveESM</a> - an adaptive sampling method based on active learning for emotions in the wild.</li>
  <li><a href="https://github.com/cheulyop/PyTEAP">PyTEAP</a> - a Python implementation of the Toolbox for Emotion Analysis using Physiological signals (<a href="https://github.com/Gijom/TEAP">TEAP</a>).</li>
  <li><a href="https://doi.org/10.5281/zenodo.3762961">K-EmoCon dataset</a> for the <a href="https://suggestbot.github.io/">SuggestBot Project</a> - a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations.</li>
</ul>

<p><strong>CV:</strong>
<a class="page-link" href="/assets/pdf/cheulyop-cv-jan21.pdf">[PDF]</a> (last updated: Jan. ‘21)</p>

  </div>



<!-- about page without profile pic -->
<!-- ---
layout: page
---


  <div class="profile col one left">
    
      <img class="one" src="/assets/img/profile.png">
    
    
  </div>


<p>My research interests include Affective Computing &amp; Human-Computer Interaction. I am also interested in applying Machine Learning to understand the relationship between our affective states and physiological signals and exploring how we can use that knowledge for real-world applications such as digital healthcare.</p>

<p>I am currently a Master’s student in the <a href="https://kse.kaist.ac.kr/">Graduate School of Knowledge Service Engineering</a> at <a href="https://www.kaist.ac.kr/en/">KAIST</a>. I recently defended my thesis and will be graduating this February.</p>

<p>I am a member of the <a href="http://ic.kaist.ac.kr/wiki/wiki.cgi?Main">Interactive Computing Lab</a> at KAIST. During my time here, I worked on the following projects:</p>
<ul>
  <li><a href="https://github.com/cheulyop/AdaptiveESM">AdaptiveESM</a> - an adaptive sampling method based on active learning for emotions in the wild.</li>
  <li><a href="https://github.com/cheulyop/PyTEAP">PyTEAP</a> - a Python implementation of the Toolbox for Emotion Analysis using Physiological signals (<a href="https://github.com/Gijom/TEAP">TEAP</a>).</li>
  <li><a href="https://doi.org/10.5281/zenodo.3762961">K-EmoCon dataset</a> for the <a href="https://suggestbot.github.io/">SuggestBot Project</a> - a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations.</li>
</ul>

<p><strong>CV:</strong>
<a class="page-link" href="/assets/pdf/cheulyop-cv-jan21.pdf">[PDF]</a> (last updated: Jan. ‘21)</p>
 -->
  </article>

  
    <div class="news">
  <h1 class="post-title">News</h1>
  
    <table>
    
    
      <tr>
        <td class="date">Dec 23, 2020</td>
        <td class="announcement">
          
            I successfully defended my thesis <em>“Towards Adaptive Sampling of Emotions in the Wild with Active Learning”</em>.

          
        </td>
      </tr>
    
      <tr>
        <td class="date">Nov 22, 2020</td>
        <td class="announcement">
          
            My past work on password sharing was mentioned in the WIRED UK article <a href="https://www.wired.co.uk/article/password-security-sharing"><em>“You need to stop sharing your passwords with your partner”</em></a> by <a href="https://www.wired.co.uk/profile/victoria-turk">Victoria Turk</a>.

          
        </td>
      </tr>
    
      <tr>
        <td class="date">Sep 8, 2020</td>
        <td class="announcement">
          
            A new paper <a href="https://www.nature.com/articles/s41597-020-00630-y"><em>“K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations”</em></a> has been published on Nature Scientific Data.

          
        </td>
      </tr>
    
      <tr>
        <td class="date">Sep 7, 2020</td>
        <td class="announcement">
          
            A new paper <a href="https://dl.acm.org/doi/10.1145/3411810"><em>“Hello There! Is Now a Good Time to Talk?: Opportune Moments for Proactive Interactions with Smart Speakers”</em></a> has been published on IMWUT.

          
        </td>
      </tr>
    
    </table>
  
</div>

  

  
    <div class="publications">
  <h1 class="post-title">Publications</h1>
  
    <!-- <header class="pub-heading">
	<h4>Manuscripts & Preprints</h4>
</header>
Note that works listed below are subject to changes during the publication process.

<ol class="bibliography"></ol> -->

<header class="pub-heading">
	<h4>Refereed Conference &amp; Journal Papers</h4>
</header>

<h3 class="year">2020</h3>
<ol class="bibliography"><li>
  
    <abbr>[<a href="https://www.nature.com/sdata/" target="_blank">Sci. Data</a>]</abbr>
  


<div id="3">
  
    <span class="title">K-EmoCon, a multimodal sensor dataset for continuous emotion recognition in naturalistic conversations</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                Narae Cha,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.co.kr/citations?hl=en&amp;user=B9HMz0EAAAAJ" target="_blank">Soowon Kang</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://kimauk.github.io/" target="_blank">Auk Kim</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.com/citations?hl=en&amp;user=MpOViugAAAAJ" target="_blank">Ahsan Khandoker</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.com/citations?user=OfAkcXkAAAAJ&amp;hl=en" target="_blank">Leontios Hadjileontiadis</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://aliceoh9.github.io/" target="_blank">Alice Oh</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="http://ibrain.kaist.ac.kr/professor/" target="_blank">Yong Jeong</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="http://ic.kaist.ac.kr/wiki/wiki.cgi?UichinLee" target="_blank">Uichin Lee</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Nature Scientific Data</em>
    
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
  
    [<a href="/assets/pdf/k-emocon.pdf" target="_blank">pdf</a>]
  
  
    [<a href="https://doi.org/10.1038/s41597-020-00630-y" target="_blank">web</a>]
  
  
  
  
  
    [<a href="https://doi.org/10.5281/zenodo.3931963" target="_blank">data</a>]
  
  
    [<a href="https://github.com/Kaist-ICLab/K-EmoCon_SupplementaryCodes" target="_blank">code</a>]
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Recognizing emotions during social interactions has many potential applications with the popularization of low-cost mobile sensors, but a challenge remains with the lack of naturalistic affective interaction data. Most existing emotion datasets do not support studying idiosyncratic emotions arising in the wild as they were collected in constrained environments. Therefore, studying emotions in the context of social interactions requires a novel dataset, and K-EmoCon is such a multimodal dataset with comprehensive annotations of continuous emotions during naturalistic conversations. The dataset contains multimodal measurements, including audiovisual recordings, EEG, and peripheral physiological signals, acquired with off-the-shelf devices from 16 sessions of approximately 10-minute long paired debates on a social issue. Distinct from previous datasets, it includes emotion annotations from all three available perspectives: self, debate partner, and external observers. Raters annotated emotional displays at intervals of every 5 seconds while viewing the debate footage, in terms of arousal-valence and 18 additional categorical emotions. The resulting K-EmoCon is the first publicly available emotion dataset accommodating the multiperspective assessment of emotions during social interactions.</p>
  </span>
  

</div>
</li>
<li>
  
    <abbr>[<a href="https://dl.acm.org/journal/imwut" target="_blank">IMWUT</a>]</abbr>
  


<div id="2">
  
    <span class="title">Hello There! Is Now a Good Time to Talk? Opportune Moments for Proactive Interactions with Smart Speakers</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                Narae Cha,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://kimauk.github.io/" target="_blank">Auk Kim</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.co.kr/citations?hl=en&amp;user=B9HMz0EAAAAJ" target="_blank">Soowon Kang</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                Mingyu Park,
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://dm.kaist.ac.kr/jaegil/" target="_blank">Jae-Gil Lee</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://scholar.google.com/citations?user=1cgtVW8AAAAJ&amp;hl=en" target="_blank">Sangsu Lee</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="http://ic.kaist.ac.kr/wiki/wiki.cgi?UichinLee" target="_blank">Uichin Lee</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</em>
    
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
  
    [<a href="/assets/pdf/hellothere.pdf" target="_blank">pdf</a>]
  
  
    [<a href="https://dl.acm.org/doi/10.1145/3411810" target="_blank">web</a>]
  
  
  
  
  
  
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Increasing number of researchers and designers are envisioning a wide range of novel proactive conversational services for smart speakers such as context-aware reminders and restocking household items. When initiating conversational interactions proactively, smart speakers need to consider users’ contexts to minimize disruption. In this work, we aim to broaden our understanding of opportune moments for proactive conversational interactions in domestic contexts. Toward this goal, we built a voice-based experience sampling device and conducted a one-week field study with 40 participants living in university dormitories. From 3,572 in-situ user experience reports, we proposed 19 activity categories to investigate contextual factors related to interruptibility. Our data analysis results show that the key determinants for opportune moments are closely related to both personal contextual factors such as busyness, mood, and resource conflicts for dual-tasking, and the other contextual factors associated with the everyday routines at home, including user mobility and social presence. Based on these findings, we discuss the need for designing context-aware proactive conversation management features that dynamically control conversational interactions based on users’ contexts and routines.</p>
  </span>
  

</div>
</li></ol>

<h3 class="year">2018</h3>
<ol class="bibliography"><li>
  
    <abbr>[<a href="https://dl.acm.org/conference/soups" target="_blank">SOUPS</a>]</abbr>
  


<div id="1">
  
    <span class="title">Share and Share Alike? An Exploration of Secure Behaviors in Romantic Relationships</span>
    <span class="author">
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              <em>Cheulyoung Park</em>,
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://corifaklaris.com/" target="_blank">Cori Faklaris</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="https://www.siyanz.com/" target="_blank">Siyan Zhao</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="http://www.alexsciuto.com/" target="_blank">Alex Sciuto</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
            
              
                <a href="http://www.lauradabbish.com/" target="_blank">Laura Dabbish</a>, 
              
            
          
        
      
      <!-- only one author -->
        
        <!-- else, for items not the last in the list -->
          
          <!-- for the last item in the list -->
            
              
                and <a href="http://www.cs.cmu.edu/~jasonh/" target="_blank">Jason Hong</a> 
              
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In Fourteenth Symposium on Usable Privacy and Security (SOUPS)</em>
    
    
    </span>
  

  <span class="links">
  
    [<a class="abstract">abstract</a>]
  
  
  
  
    [<a href="/assets/pdf/soups2018-park.pdf" target="_blank">pdf</a>]
  
  
    [<a href="https://www.usenix.org/conference/soups2018/presentation/park" target="_blank">web</a>]
  
  
  
  
  
  
  
    [<a href="https://www.cylab.cmu.edu/news/2018/09/07-romantic-couples-security.html" target="_blank">press</a>]
  
  </span>

  <!-- Hidden abstract block -->
  
  <span class="abstract hidden">
    <p>Security design choices often fail to take into account users’ social context. Our work is among the first to examine security behavior in romantic relationships. We surveyed 195 people on Amazon Mechanical Turk about their relationship status and account sharing behavior for a cross-section of popular websites and apps (e.g., Netflix, Amazon Prime). We examine differences in account sharing behavior at different stages in a relationship and for people in different age groups and income levels. We also present a taxonomy of sharing motivations and behaviors based on the iterative coding of open-ended responses. Based on this taxonomy, we present design recommendations to support end users in three relationship stages: when they start sharing access with romantic partners; when they are maintaining that sharing; and when they decide to stop. Our findings contribute to the field of usable privacy and security by enhancing our understanding of security and privacy behaviors and needs in intimate social relationships.</p>
  </span>
  

</div>
</li></ol>


  
</div>
  

  
    <div class="research">
  <h1 class="post-title">Research</h1>
  
    <ul>
  <li><strong><a href="http://ic.kaist.ac.kr/wiki/wiki.cgi?Main">Interactive Computing Lab</a></strong>, KAIST (Aug ‘18 ~ present)
    <ul>
      <li>Affective Computing, HCI (Advised by <a href="http://ic.kaist.ac.kr/wiki/wiki.cgi?UichinLee">Prof. Uichin Lee</a>)</li>
    </ul>
  </li>
  <li><strong><a href="http://cmuchimps.org/">CHIMPS Lab</a></strong>, CMU (Jul ‘17 ~ Jul ‘18)
    <ul>
      <li>Usable Privacy and Security, HCI (Advised by <a href="http://www.cs.cmu.edu/~jasonh/">Prof. Jason Hong</a>)</li>
    </ul>
  </li>
  <li><strong><a href="https://www.cmu.edu/dietrich/psychology/infant-language-learning-lab/">Infant Language and Learning Lab</a></strong>, CMU (May ‘16 ~ Dec ‘16)
    <ul>
      <li>Developmental Psychology, Cognitive Psychology (Advised by <a href="https://www.cmu.edu/dietrich/psychology/people/core-training-faculty/thiessen-erik.html">Prof. Erik Thiessen</a>)</li>
    </ul>
  </li>
</ul>

  
</div>
  

  
    <div class="courses">
  <h1 class="post-title">Teaching</h1>
  
    <!-- <header class="pub-heading">
	<h4>Machine Learning</h4>
</header> -->

<ul>
  <li><a href="https://ita.kaist.ac.kr/"><strong>KAIST IT Academy</strong></a> - <strong>Image Deep Learning with PyTorch</strong> (<ins>Feb ‘20</ins>, co-instructed with <a href="https://linkedin.com/in/jisuhan">Jisu Han</a>)
    <ul>
      <li>A 30-hour course on the basics of deep learning using PyTorch with a focus on image processing, taught in Korean &amp; English.</li>
    </ul>
  </li>
</ul>

<!-- <header class="pub-heading">
	<h4>Programming</h4>
</header> -->

<ul>
  <li><a href="https://ita.kaist.ac.kr/"><strong>KAIST IT Academy</strong></a> - <strong>Python Programming Basics</strong> (<ins>Oct ‘19</ins> &amp; <ins>Jan ‘20</ins>)
    <ul>
      <li>A 30-hour course on the fundamentals of programming with Python, taught in Korean &amp; English.</li>
    </ul>
  </li>
</ul>

  
</div>
  

  
    <div class="social">
  <span class="contacticon center" style="font-size: 16px">
    <a href="mailto:%63%68%65%75%6C%79%6F%70@%67%6D%61%69%6C.%63%6F%6D">email</a>
    <!-- <a href="https://orcid.org/0000-0003-0414-272X" target="_blank" title="ORCID">ORCID</a> -->
    <a href="https://github.com/cheulyop" target="_blank" title="GitHub">github</a>
  </span>

  <!-- <span class="contacticon center" style="font-size: 24px">
    <a href="mailto:%63%68%65%75%6C%79%6F%70@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope fa-sm"></i></a>
    <a href="https://orcid.org/0000-0003-0414-272X" target="_blank" title="ORCID"><i class="ai ai-orcid ai-xs"></i></a>
    
    
    
    <a href="https://github.com/cheulyop" target="_blank" title="GitHub"><i class="fab fa-github fa-sm"></i></a>
    
    
    
    
    
    
  </span> -->

  <div class="col three caption">
    
  </div>
</div>

  

</div>

      </div>
    </div>

    <!-- <footer>

  <div class="wrapper">
    &copy; Copyright 2021 Cheul Young Park.
    
    
        Last updated: 2021-01-11.
    
  </div>

</footer>
 -->

    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>





<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/assets/css/academicons.min.css">


<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-163428563-1', 'auto');
ga('send', 'pageview');
</script>



  </body>

</html>
